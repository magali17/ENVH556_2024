---
title: "Week 8 Lab:  Measurement Error"
author: "Instructors for ENVH 556 Autumn 2024"
format:
  html:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    self-contained: true 
editor_options: 
  chunk_output_type: console
---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.

```{r setup, include=FALSE}
#-----setup-----

# set knitr options
knitr::opts_chunk$set(echo = TRUE)

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}
#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(tidyverse, knitr, tictoc, broom, parallel, Hmisc)

```


# Introduction and Purpose

The purpose of this lab is to **better understand pure classical and Berkson measurement error**, and optionally to 1) replicate the results reported in Szpiro et al (2011) and 2) use the replication process to gain a deeper understanding of the role of exposure prediction in inference from epidemiological studies. An additional benefit of this lab is that you will **gain some experience with simulation studies**, a skill that is generally applicable to a wide range of problems.


In the **pure measurement error (ME)** simulation approach, we directly generate exposures (true and mismeasured) for subjects using predefined pure ME definitions.

In contrast, the **"like" setting** represents a scenario where exposure measurements (whether true or mismeasured) are not available for all subjects. Instead, exposures must be predicted. In this case, we simulate the prediction process by collecting samples from a set of monitoring locations and using these data to estimate exposures at the subject locations. This approach "blends" Berkson and Classical measurement errors.

Using both exposure approaches, we calculate the estimated association between the exposure and outcome of interest. 


## Overview of Pure Berkson and Pure Classical Measurement Error

In this lab, we will focus on pure measurement error properties. The steps are as follows:

1. **Simulate Exposure and Health Data:**
   - Generate data for a single population to represent true exposure and health outcomes.

2. **Introduce Exposure Measurement Error:**
   - Create three versions each of:
     - Pure Berkson measurement error.
     - Pure Classical measurement error.

3. **Conduct Health Analysis:**
   - Use the exposures in a health analysis to estimate the association between the exposure and health outcome $\beta$.

4. **Summarize and Analyze $\beta$ :**
   - Store $\beta$ estimates over multiple simulations (e.g., 1,000 repetitions).
   - Summarize the estimates to evaluate how different exposure estimates influence this target parameter.

We will use the function provided in the file **mesim_pure.R** to facilitate the exercise. 
This function generates true exposure and health outcome data in a group of subjects, along with modifications of the true exposure to give mismeasured exposures, both pure Berkson and pure classical.


## Optional Exercise: Overview of Berkson-Like and Classical-Like Measurement Error

As an optional additional exercise, we will explore Berkson-Like and Classical-Like measurement error. The steps are as follows:

1. **Define Two Groups of Locations:**
   - Separate the data into:
     - Monitoring (or sampling) locations.
     - Subject residence locations.
   - In an occupational study, these groups could represent:
     - A worker cohort with exposure monitoring data.
     - A cohort with health outcome data.
   - Note: In the current application, these groups are mutually exclusive, but in other applications, there may be overlap (e.g., workers with health data may also have exposure data).

2. **Simulate Exposure, Health Outcome, and Exposure Monitoring Data:**
  - Generate true exposure data for subjects given measured covariates      
  - Generate health outcomes for subjects based on their **true exposures**.
  - Generate true exposures at monitoring locations given measured covariates

3. **Predict Subject Exposures (i.e., Introduce Measurement Error):**
   - Use a regression model trained on the monitoring data to **predict exposures for subjects** (i.e., out-of-sample predictions) based on measured exposures and geographic covariates at monitoring locations.

4. **Conduct Health Analysis:**
   - Use the exposure predictions in a health analysis to estimate the association between the exposure and health outcome $\beta$.

5. **Summarize and Analyze $\beta$:**
   - Store $\beta$ estimates over multiple simulations (e.g., 1,000 repetitions).
   - Summarize these estimates to evaluate the role of different exposure models on this target parameter.

The function provided in the file *mesim_like.R* will facilitate this exercise. This is a more complex version of measurement error based on the work done by Szpiro et al. It **generates the exposure and health data, computes predictions**, and estimates the quantities of interest that can be summarized, as described in Szpiro et al.  
 
 
## Simulation Studies

Simulation studies are a form of experimental study most often used to understand properties of statistical estimators. (Simulations are also used for other purposes, such as in study planning as a tool to estimate study power.) The basic idea behind a simulation study is that data are generated using an assumed data-generating model. These data are then used to estimate one or more quantities of interest using one or more approaches to analysis. (The analysis model(s) may or may not correspond to the data-generating model; this misalignment can be done on purpose to understand some important feature of the role of data analysis under varying data-generating conditions.) The resulting estimates are then summarized and compared to quantify their properties. The simultaneous strength and weakness of simulation studies is that the true data-generating model is known. Thus, we can evaluate exactly how a statistic performs under a given set of assumptions. Of course, in the real world, we never know whether our assumed model holds.

**Comments on simulating data**: Computer simulations give realizations from probability distributions. They are based on pseudo-random numbers – these numbers are generated by the computer to behave like random numbers, but they are deterministic. Thus, if you want to be able to replicate a specific simulation exactly each time it is run, you set the seed or starting value for the pseudo-random number generator. In R, we **use the `set.seed()` command with an arbitrary number inside the parentheses to support reproducibility**.


# Pure Berkson and classical measurement error 

We use **simulation** to get the statistical properties of our results: To simulate this, we replicate steps *1. Data Generation* and *2. Data Analysis* `n_iter` number of times. In practice, the value for `n_iter` should be high, 1,000 or more. For testing, it is good practice to use a MUCH smaller number of replicates, such as `n_iter=10`.

1. **Data generation**: Generate the exposure and outcome data on subjects for one "cohort". (Note: In a simulation study, we can assume the true exposure is known. In real life, this is (essentially) never true.)
    a. Assume a **cohort of a fixed size**, e.g., `n_subj = 10,000`.
    b. Our **true exposure**, `x`, is built from a model that has 3 covariates and an error term:
        $$x = \alpha_0 + \alpha_1 s_1 + \alpha_2 s_2 + \alpha_3 s_3 + \eta$$
    c. **Berkson ME exposures**: Exposures with Berkson error allow one to observe part of the true exposure (**we illustrate that here by only including a subset of the predictors**). Connected to our true exposure model, we define `Berk_1` to have 1 covariate ($s_1$), `Berk_2` to have 2 ($s_1$, $s_2$), and `Berk_3` to have 3 ($s_1$, $s_2$, $s_3$):
       $$
       \begin{align}
       Berk_1 &= \alpha_0 + \alpha_1 s_1 \\
       Berk_2 &= \alpha_0 + \alpha_1 s_1 + \alpha_2 s_2 \\
       Berk_3 &= \alpha_0 + \alpha_1 s_1 + \alpha_2 s_2 + \alpha_3 s_3 
       \end{align}
       $$ 
    d. **Classical ME exposures**: Exposures with classical measurement error have independent **noise added to the true exposure**. We define `class1` to have additional error added, while `class2` has more added error, and `class3` has even more added error:
       $$ 
       \begin{align}
       class_1 &= x + e_1 \\
       class_2 &= x + e_1 + e_2 \\
       class_3 &= x + e_1 + e_2 + e_3 
       \end{align}  
       $$
    e. **Health outcome model**: We assume a simple linear regression for health outcome `y`. Note that the true health model is conditional on the true exposure, NOT one of our mismeasured exposures defined above.
       $$ y = \beta_0 + \beta_1 x + \epsilon $$
       
2. **Data analysis**: For a single cohort, use all exposures in health analyses, one health analysis for each exposure; save health results.
    a. First, estimate health effect given true exposure.
    b. Then consider health effect estimates using the 3 Berkson error exposures.
    c. Then consider health effect estimates using the 3 classical error exposures.
    
3. **Summarize the results** of the exposures and health effect estimates after the program completes.

The following chunk takes you through the data-generation and data analysis steps using simulation. In the following chunk, we help you get started with summarizing the results. We also demonstrate the use of the `tictoc` package for timing the duration of the simulation and provide optional code (commented out) for using parallel processing.

```{r measurement.error.simulation_pure}
#-----measurement error simulation for pure Berkson & classical-----

# Steps to accomplish the above:  
#   I. define function that creates the data for one iteration of n_iter
#   II. repeat over n_iter iterations.  We will use lapply(); the replicate
#       command is an alternative
#   III. transform the list structure to one easier to analyze
#   IV. Summarize the results

# Step I: Read in the functions in the mesim_pure.R code
source("mesim_pure.R")

# Set the seed
set.seed(556)

# define the number of simulation iterations to use
n_iter <- 10

# start timing for single-core processing
tic("single core pure ME processing time")

# Step II:  Iterate
# lapply runs the function me_pure n_iter times. 
# For each iteration: for X subjects, generate a true exposure, exposure with measurement error, and an outcome &  
result_pure_sp <- lapply(seq_len(n_iter), function(x) me_pure(n_subj = 10000)) %>% 
  
  # bind dataframes for each iteration together (Step III)
  bind_rows(.id = "replicate")
  
toc()

# see results - each predictor produces slightly different parameters of interest
result_pure_sp 

# # Bonus topic:  parallel processing - use mclapply() with an mc.cores >1
# # The above application uses single-core processing.  Multi-core (or parallel)
# # processing can greatly speed up long programs.  We use the tictoc
# # function to determine how long a procedure works.  We use multi-core versions
# # of functions to allow the parallel processing as in the following steps:
# # Multi-core processing - Note: mclapply() does not work in Windows
# if (.Platform$OS.type != "windows") {
#   
#   # start timing for multi-core processing
#   tic("multi-core pure ME processing time")  
#   
#   # Step II:  Iterate
#   result_pure_mp <- mclapply(1:n_iter, function(x) me_pure(pure_data),
#                              mc.cores = 8,
#                              mc.set.seed = TRUE,
#                              mc.preschedule = TRUE) %>% 
#     
#     # bind dataframes for each iteration together (Step III)
#     bind_rows(.id = "replicate")
#   
#   toc()
# }

# Note: There are slight differences between the single and multi-core versions
# which is due to internal seed-setting in the multi-core version.

# Step IV. (In the next chunk) Summarize the results

```

The next chunk is to **summarize the results** using basic descriptive tools and plots.  You should build upon the tools you have learned earlier in the quarter to conduct descriptive analyses and develop plots.  See some of the text in the practice session below for some more thoughts on what quantities you want to
summarize.  Also refer to the Table and Figures in Szpiro et al 2011.

```{r summarize.pure.ME.results}
#-----summarize pure ME results-----

# Summarize the results: find mean and sd of "b1" for all 7 predictors along
# with the E(se(beta1)), mean R2, and average variance of each exposure (ideally
# reported on the SD scale but reported on the variance scale below). Students
# should also think about graphical displays such as density plots.

# Note: if you're using the multicore results, change `result_pure_sp` to 
# `result_pure_mp` 

res_pure <- result_pure_sp %>% 
  # for each predictor (i.e., true exposure or exposure with error)  
  group_by(predictor) %>% 
    summarise(N = n(), # number of total iterations conducted
              `E(b1)` = mean(b1), # mean of the betas across those iterations
              sd_b1 = sd(b1), # SD (variability) of those betas
              `E(se_b1)` = sqrt(mean(seb1^2)), # mean beta SE
              `RMSE_b1` = sqrt(var(b1)+(mean(b1-2)^2)),
              `E(R2_exp(reg))` = mean(R2_W_reg), # mean exposure model reg-based R2
              `E(R2_exp(MSE))` = mean(R2_W_MSE), # mean exposure model MSE-based R2
              `E(exp_var)` = mean(exp_var), # mean exposure variability simulated
              .groups = "drop" 
              ) %>%
    arrange(`E(exp_var)`) 
    
# # The following commented out version adds in 95% coverage statistics
# res_pure <- result_pure_sp %>%
#     rowwise %>% 
#     mutate(is_95cov = between(2, b1 - 1.96*seb1, b1 + 1.96*seb1)) %>%
#     group_by(predictor) %>% 
#     summarise(N = n(), 
#               `E(b1)` = mean(b1),
#               sd_b1 = sd(b1),
#               Coverage = sum(is_95cov)/N,
#               `E(se_b1)` = sqrt(mean(seb1^2)),
#               `RMSE_b1` = sqrt(var(b1)+(mean(b1-2)^2)),
#               `E(R2_exp(reg))` = mean(R2_W_reg),
#               `E(R2_exp(MSE))` = mean(R2_W_MSE),
#               `E(exp_var)` = mean(exp_var),
#               .groups = "drop" 
#               ) %>%
#     arrange(`E(exp_var)`) 
    
# Display results 
# Note: This presentation removes N from the table and puts it in the caption.
# This is good practice since we don't want columns in our tables with constant
# values.

kable(
  res_pure #%>% select(-N) # drop N when reporting if it is constant (keeping here for illustration purposes)
  ,
  digits = 3,
  caption = paste0( 
    "Health effect estimate properties and exposure variation based on simulations with ",
                    unique(res_pure$N), " iterations." )
)

```

## Plots of one dataset showing the best fit lines for true vs. mismeasured exposures

Note: For clarity of display we are presenting only 1,000 subjects.  However, the sample size of the subject dataset impacts the certainty of the parameter estimates, so the slopes shown in this figure have more uncertainty than the slopes estimated in the simulation study with 10,000 subjects.  (Why?)

**How do the scatter plots and fitted lines for the true exposure x and y compare to those when x has measurement error?** 

```{r plot.pure.ME.data, message=FALSE}
#-----plot pure ME data-----

# Set the seed
set.seed(556)

# create one example of the pure measurement error dataframe
#pure_data <- me_pure_data(n_subj = 10000)
pure_data <- me_pure_data(n_subj = 1000)

head(pure_data)

# for reference, get the "true" coefficients of the relationship between exposure and outcome 
coefs_true <- lm(y ~ x, pure_data)$coefficients

coefs_true

# create temporary datafame for ggplot
temp <- pure_data %>% 

  # make dataset longer
  pivot_longer(cols = c(contains("Berk_"), contains("class_"), "x"), 
               names_to = "error_type", 
               values_to = "error_exposure")

# make plot
ggplot(data = temp, aes(x = error_exposure, y = y)) + 
  
  # different plots by error type
  facet_wrap(~error_type) +
  
  # points
  geom_point(shape = "o", alpha = 0.6) +
  
  # Best fit line given true exposure x
  geom_abline(aes(intercept = coefs_true[1], slope = coefs_true[2], 
              linetype = "dashed", color = "black")) +
  
  # best fit line given mismeasured exposure
  geom_smooth(method = lm, se = FALSE, aes(linetype = "solid", color = "red")) +
  
  # specify legend values manually
  scale_color_manual(name = "Exposure Type",
                     values = c("black", "red"),
                     labels = c("True", "Measured")) +
  
  scale_linetype_manual(name = "Exposure Type",
                        values = c("dashed", "solid"),
                        labels = c("True", "Measured")) +

  # labels
  labs(x = "Exposure with Various Types of Pure Measurement Error", 
       y = "Outcome y") +
  
  # theme
  theme_bw() + 
  theme(legend.position = "bottom")
  

```


# Optional Exercise: Berkson**-like** and classical**-like** measurement error

1.  **Data generation**:  Generate the exposure and outcome data on subjects for
    one "cohort".  (Note:  In a simulation study we can assume the true exposure 
    is known.  In real life this is (essentially) never true.)
    a.  Assume a **cohort of a fixed size**, e.g. `n_subj = 10,000`.
    b.  Our **true exposure**, `x`, is built from a model that has 3 covariates
        and an error term:
        $$x = \alpha_0 + \alpha_1 s_1 + \alpha_2 s_2 + \alpha_3 s_3 + \eta$$
    c. **Predicted exposure using the correctly specified exposure model (full
       model)**:  Predict exposures in the monitoring data using the same correctly
       specified exposure model as determines the true exposure:
       $$ \hat{x}_{full} = \hat{\alpha}_0 + \hat{\alpha}_1 s_1 + \hat{\alpha}_2 s_2 + \hat{\alpha}_3 s_3 $$   
    d. **Predicted exposure using the mis-specified exposure model (reduced
       model)**:  Predict exposures in the monitoring data using a mis-specified
       exposure model as compared to the one that determines the true exposure.
       This one is simpler than the one that determines the true exposure:
       $$ \hat{x}_{red} = \hat{\alpha}_0 + \hat{\alpha}_1 s_1 + \hat{\alpha}_2 s_2 $$ 
    
    e. **Health outcome model**:  We assume a simple linear regression for
       health outcome `y`.  Note the true health model is conditional on the true
       exposure, NOT one of our mismeasued exposures defined above.
       $$ y = \beta_0 + \beta_1 x + \epsilon $$
    
2.  **Data analysis**:  For a single cohort, use all exposures in a health
    analyses, one health analysis for each exposure; save health results
    a. First estimate health effect given true exposure
    b. Then consider health effect estimates conditional on exposures predicted
       using a model developed from the monitor dataset that is representative of
       the subject population.  Consider both the correctly specified exposure
       model (full) and the mis-specified exposure model (reduced).
    c. Then consider health effect estimates conditional on exposures predicted
       using a model developed from the monitor dataset that is *not*
       representative of the subject population.  Consider both the correctly
       specified exposure model (full) and the mis-specified exposure model
       (reduced).
    
3.  **Summarize the results** of the exposures and health effect estimates after
    the program completes

```{r ME.simulation.berkson.like}
#-----ME simulation for Berkson-like & classical-like measurement error-----

# Steps to accomplish the above:  
#   I. define function that creates the data for one iteration of n_iter
#   II. repeat over n_iter iterations.  We will use lapply(); the replicate
#       command is an alternative
#   III. transform the list structure to one easier to analyze
#   IV. Summarize the results

# # Step I: Read in the function in the mesim_like.R code
source("mesim_like.R")

# Set the seed
set.seed(521)

# define the number of simulation iterations to use
# Note: with a small number of iterations, the results may not be consistent
# with the results reported in the paper.  They will also vary more than you
# might expect if you change the seed. (Why?)
n_iter <- 100

# start timing for single-core processing
tic("single-core Berkson-like ME timing")

# Step II:  Iterate
result_like_sp <- lapply(seq_len(n_iter), function(x) me_like(n_subj = 10000)) %>% 
  
  # bind list elements (Step III)
  bind_rows(.id = "replicate")

toc()

# # Bonus topic:  parallel processing (see the pure version for explanation)
# # Multi-core processing - Note: mclapply() does not work in Windows
# if (.Platform$OS.type != "windows") {
# 
#   # start timing for single-core processing
#   tic("multi-core Berkson-like ME timing")
# 
#   # multi-core processing
#   result_like_mp <- mclapply(1:n_iter, function(x) me_like(),
#                                            mc.cores = 8,
#                                            mc.set.seed = TRUE,
#                                            mc.preschedule = TRUE) %>%
#   # bind list elements (Step III)
#   bind_rows(.id = "replicate")
# 
#   toc()
# }

# Step IV. (In the next chunk) Summarize the results

```

The next chunk is to summarize the results using basic descriptive tools and
plots.  We only show a simple version of part of the summary needed.

In the example used in this lab, Version 1 is for the third covariate having the
same variation at the subject and monitor locations while Version 2 is where
there is much less variation at the monitor locations than at the subject
locations.

```{r summarize.like.ME.results}
#-----summarize like ME results-----

# This repeats similar results as developed for the pure ME case.
# Note:  Students need to analyze the other features and develop plots

# Note: if you're using the multicore results, change `result_like_sp` to 
# `result_like_mp`

# summarize the results across many simulations
res_like <- result_like_sp %>% 
  group_by(exposure_vars) %>% 
  summarise(N = n(),
            `R2_W_reg` = mean(R2_W_reg),
            `R2_W_MSE` = mean(R2_W_MSE),
            `E(exp_var)` = mean(exp_var),
            `E(a3)` = mean(a3hat),
            `E(se(a3))` = sqrt(mean(a3var)),
            `E(b1)` = mean(b1),
            `SD(b1)` = sd(b1),
            `E(se_b1)` = sqrt(mean(seb1^2)),
            `RMSE_b1` = sqrt(var(b1)+(mean(b1-2)^2)),
            .groups = "drop"
            ) %>% 
  arrange(exposure_vars) 

# Display results
kable(res_like #%>% select(-N) #keeping N here for illustrative purposes
      , 
      digits = 3,
      caption = paste0(
              "Exposure R2 and health effect estimate properties based on simulations with ", 
                       unique(res_like$N), " iterations.") 
      )

```

# Practice Session

1.  Make sure all the code files you will need are available inside your project folder.
2.  In lab we will read through the `mesim_pure` function together to make sure
    you understand each step.
    a. You can practice on one iteration so you better understand its results
       by running `me_pure()`.
    b. While it will take more work, you may wish to try each of the
       data-generating and fitting steps manually to solidify your understanding.
3.  Run a small number of simulations for practice (e.g. 10-50).
4.  Develop some procedures to summarize the results.  Think about what
    variables you will need to summarize to be able to show results using a format
    similar to those reported in the Table and Figures (1 and 2) of Szpiro et al.
    a.  Considering the summary statistics for $\hat{\beta}_x$, how were the
        standard deviation, RMSE, and E(SE) in the Table estimated?  (Hint: the
        standard deviation and RMSE are estimated using $\hat{\beta}_x$, while 
        E(SE) is shorthand notation for the “expected” (or average) standard 
        error of $\hat{\beta}_x$ estimated using the variance of $\hat{\beta}_x$.)
    b.  You will need to calculate your own coverage probabilities.  To do this,
        calculate a 95% CI for (each) $\hat{\beta}_x$ in each simulation and
        determine whether or not the true value for $\beta_x$ (=2) is covered by
        that CI.  (Hint:  Generate a new variable that is an indicator variable 
        for your result and summarize this over all simulations.)
    c.  You may also want to consider summarizing one full dataset used in one
        simulation iteration.
5.  For your homework, we’d like you to run a large number of simulations (e.g.
    1,000 or more).  Then summarize these in a table and one or more figures,
    thinking creatively about what you can show in these data to highlight the
    insights you have gained.
6.  For the optional extra credit part of the homework, we would like you to
    study both 1) the two conditions shown in the Szpiro et al (2011) Table in 
    order to replicate the table, and 2) in addition choose (at least) one more 
    pair of conditions to investigate.


# Homework Exercises 

## Standard Lab

1.  Create your own version of the Szpiro et al Table using the results from
    your simulation study.  Make sure to show some exposure characteristics as 
    well as characteristics of the health effect estimates for pure Berkson and 
    classical measurement error, respectively.  Discuss your understanding of 
    these results.
2.  Develop one or more figures to show your insights into pure Berkson and classical measurement error.  Consider figures that show:
    a. The relationship in one simulated dataset.
    b. The distribution of the health effect parameter estimate across simulations.
3.  In your lab write-up:  
    a. Make sure you describe clearly the conditions you studied in your
       simulation study.   State the assumptions used in the simulation.  (Note:
       If you wish you may change the underlying assumptions, but if you do, make
       sure you convey what you did completely clearly in your write-up.)
    b. Show some informative figures to help you make your points.  Think about
    both the exposure side and the health effect estimate side of the simulation
    study results.  If you show figures that summarize a single simulated
    dataset vs. figures that summarize results across all simulated datasets,
    indicate your understanding of how they are related.
    c. Discuss your understanding of pure Berkson and classical measurement
       error and connect that understanding to the quantities you simulated and
       estimated in the simulation study.

## Optional Extra Credit lab on exposure prediction & measurement error (+1 point)  

Please include a note with your lab if you decide to complete this extra credit portion of the lab. 
    
1.  Reproduce the Szpiro et al (2011) Table using the same two conditions for
    the variance of $s_3$ in your simulation study.  Discuss your understanding 
    of these results.
2.  Choose (at least) one more pair of conditions to investigate in a second
    simulation study by varying one or more of: the sample sizes for the monitoring
    data and/or the health study, and/or the relative variability of $s_3$ in the
    two sets of monitoring data.  (One option is to try to reproduce Figure 3 in 
    the paper.)
    a.  Describe the conditions you studied in your second simulation study.
        Summarize your results and discuss them.
    b.  With respect to both 1. and 2. above, if you have gained any additional
        insights from looking at any of the other statistics available from the
        simulation but not reported in the paper, please incorporate these insights
        into your comments.
3.  Think about the title of this paper and your intuition about the role of
    exposure prediction on inference about health effects (or at least any intuition
    you may have had prior to reading this paper).  Why do you think the authors 
    had difficulty convincing the journal editors to publish this paper?  Do you 
    think the inclusion of the sample code in the supplement and your work trying 
    to replicate the simulation studies inspired by this paper helps make the results
    more convincing?
4.  In a simulation study we can use the true exposure at subject locations as
    out-of-sample test data for assessing the quality of the exposure prediction
    model as was done here.  Typically we never know the true exposure and also 
    we don’t have detailed exposure measurements on all the subjects who also provide
    health data.  Thus in applications we often resort to assessing the quality 
    of the exposure predictions in the exposure monitoring data alone, using .e.g.
    hold-out datasets or cross-validation strategies. In the settings evaluated in
    this simulation study, speculate on how our inference and conclusions could 
    be impacted by using the monitoring data instead of the subject data for the
    assessment of the performance of the exposure model.   (If you wish, you may 
    try cross-validating the exposure model results and comparing these to the 
    true out-of-sample results you have already reported.)


# Code Appendix

## Session Information

```{r session.info}
#-----session info: beginning of Code Appendix-----

sessionInfo()

```

## Code in the R Markdown file

```{r appendix.code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}
#-----appendix code-----

```

## User-written functions loaded in the R Markdown environment

```{r functions.used.in.this.Rmd, eval = TRUE}
#-----functions used in this Rmd-----

# Show the names of all functions used (loaded in the current environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lsf.str() %>% set_names() %>% map(get, .GlobalEnv)

```
